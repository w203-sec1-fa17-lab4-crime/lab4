---
title: "Lab 4"
author: "Shan He, Joanna Huang, Tiffany Jaya"
date: "18 December 2017"
output: pdf_document
---

TODO: As this is a policy exercise, you should do your best to address the campaign’s questions from a causal perspective. At the same time, you should clearly explain the limitations of your analysis, and provide discussion around whether your estimates suffer from endogeneity bias.

# Introduction 

TODO: A brief introduction

# Exploratory Data Analysis

TODO: An initial exploratory analysis. Detect any anomalies, including missing values, top-coded or bottom- coded variables, etc.


```{r}
library(car) # lm
library(ggplot2) # ggplot
library(lmtest) # bptest
library(plm) # vcovHC
library(sandwich) 
library(stargazer) # stargazer
library(tidyr) # gather
```

```{r}
data <- read.csv("crime.csv")
summary(data)
```

```{r}
# sample size n = 90
nrow(data)
# number of variables = 26
ncol(data)
```

```{r}
# verify number of missing values = 0
colSums(sapply(data, is.na))
```

```{r}
# plot every variable except x, county, year
plot.data <- data[!(names(data) %in% c("X", "county", "year"))]
ggplot(gather(plot.data), aes(value)) + 
       facet_wrap(~key, scales="free") + 
       geom_histogram()
```
# Observations
- All observations were recorded for the year of 1987. 
- $urban$, $central$, $west$ are categorical variables.
- Most wages variables (wcon, wfed, wfir, wmfg, wser, wsta, wtrd) have a positively skewed distribution which may be due to few number people getting paid above the average. 
- wtuc, wloc appear to be normally distributed.
- prbarr, prbconv appear to be positively skewed while prbpris is more negatively skewed.
- avgsen appears to be positively skewed.
- crmrte, density, polpc, prbconv: The histogram indicates that these variables have positive skews. Given the variables have a meaningful zero-point, we can take the log for a more normal distribution.

```{r}
# taking the log of the data excluding x, county, year and categorical variables
plot.log.data <- log(data[!(names(data) %in% c("X", "county", "year", "urban", "central", "west"))])
ggplot(gather(plot.log.data), aes(value)) + 
       facet_wrap(~key, scales="free") + 
       geom_histogram()
```

*Observations after taking logs:*
It looks like the log transformation of variables cmrte, mix, prbconv, wfed, wfir, wmfg, and wsta have made the distributions quite normal. This will help ensure the errors of the model are normal.

```{r}
# just list outliers for variables that are non-categorical
subset.data <- data[!(names(data) %in% c("X", "county", "year", "central", "urban", "west"))]

# list all outliers
list_all_outliers <- function (var) {
  outliers <- sort(boxplot.stats(var)$out)
  return(paste(length(outliers), ": ", paste(outliers, collapse=", ")))
}
lapply(subset.data, list_all_outliers)
```

Glaring outliers:
1. Probability > 1 for prbarr and prbconv
2. Outlier 2177 in wser which happens to be in central instead of urban (weird)

```{r}
# number of urban
sum(data$urban == 1)
# number of central
sum(data$central == 1)
# number of west
sum(data$west == 1)
```

```{r}
# determine if a county can have multiple categorical label: urban, central, west
# 5 are both urban and central, 1 is urban and west, total of 6 labeled with more than one categorical label
all.county <- c(urban.county, central.county, west.county)
all.county[duplicated(all.county)]
```
Shan: this part didn't work for me. Here is another way to check:
```{r}
unique(cbind(data$urban,data$central,data$west))
```

```{r out.width="20%", fig.align="center", fig.show="hold"}
# in urban area, is there higher ...?
independents <- c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen", "polpc", "density", "taxpc", "pctmin80", "mix", "pctymle")
mapply(function(var, var.name) {
  boxplot(var ~ urban, data=data, names=c("non-urban", "urban"), main=var.name)
}, subset(data, select=independents), independents)
```

```{r}
# repeat of above but checking out all the wages
mapply(function(var, var.name) {
  boxplot(var ~ urban, data=data, names=c("non-urban", "urban"), main=var.name)
}, subset.data, names(subset.data))
```

The following assertions should be understood in the context of a limited dataset of 8 urban, 34 central and 21 west counties. 

In urban areas, there is higher

* crime rates
* average sentence
* police per capita
* density
* tax revenue per capita
* percentage of minority (slightly higher but not much)
* percentage of young male (slightly higher but not much)
* wages tend to be higher overall

In urban areas, there is lower

* probability of arrest
* probability of conviction
* face to face confrontation (slightly lower but not much)

In urban areas, there is about the same

* probability of prison sentence

```{r out.width="20%", fig.align="center", fig.show="hold"}
# comparing central and west, is there higher ...?
central.data <- data[data$central == 1, ]
subset.central.data <- subset(central.data, select=independents)
west.data <- data[data$west == 1, ]
subset.west.data <- subset(west.data, select=independents)
mapply(function(central.var, west.var, var.name) {
  boxplot(central.var, west.var, names=c("central", "west"), main=var.name)
}, subset.central.data, subset.west.data, independents)
```

```{r}
# repeat of above but checking out all the wages
central.data <- data[data$central == 1, ]
subset.central.data <- subset(central.data, select=names(subset.data))
west.data <- data[data$west == 1, ]
subset.west.data <- subset(west.data, select=names(subset.data))
mapply(function(central.var, west.var, var.name) {
  boxplot(central.var, west.var, names=c("central", "west"), main=var.name)
}, subset.central.data, subset.west.data, names(subset.data))
```


Comparing central vs west, 

central has higher

* crime rate
* density 
* tax revenue per capita
* percentage of minority
* percentage of young male
* wages in general are higher except for wtuc

west has higher

* average sentence

central and west are about the same

* probability of arrest
* probabilitiy of conviction
* probability of prison sentence
* police per capita
* face to face confrontation (central has slightly higher)

# Model Variable Exploration

```{r}
# covariance matrix of variables
cor(data[!(names(data) %in% c("X", "county", "year", "urban", "central", "west"))])
```

```{r}
# potential dependent variable: crime rate
hist(data$crmrte)
hist(log(data$crmrte))
# potential independent variables
# 1. density
hist(data$density)
hist(log(data$density))
# 2. taxpc
hist(data$taxpc)
hist(log(data$taxpc))
# log didn't quite change much because of one outlier
# 3. pctmin80
hist(data$pctmin80)
hist(log(data$pctmin80))
# better leave variable as is because applying a log skewed the distribution
# 4. pctymle
hist(data$pctymle)
hist(log(data$pctymle))
# log didn't quite change much because of one outlier
# 5. avgsen
hist(data$avgsen)
hist(log(data$avgsen))
# already verified that these variables are not correlated to each other
```

Additional considerations:

- Sum of all wages: Summing the wage variables across sectors may identify whether income inequality between counties may explain the difference in crime rates. Initial thoughts are that sum of wages (higher incomes) lead to lower crime rates, so a linear relationship between sum of wages and crimes committed per person is expected with a negative coefficient.

- log(prbarr) Probability of arrest: There is the possibility that the probability of arrest is positively correlated with crime rate, as higher numbers of arrest can increase the number of crimes recorded.

- tax revenue per capita (taxpc): Lack of government-funded resources can take the form of a lack of educational opportunities or employment options, thus leading to rise in crime rates.

# Model Building Process

TODO: A model building process, supported by exploratory analysis. Your EDA should be interspersed with, and support, your modeling decisions. In particular, you should use exploratory techniques to address
* What transformations to apply to variables and what new variables should be created.
* What variables should be included in each model
* Whether model assumptions are met

$density
[1] "8 :  3.93455100059509, 4.38875865936279, 4.8347339630127, 5.12442398071289, 5.6744966506958, 6.28648662567139, 6.42718458175659, 8.82765197753906"

$taxpc
[1] "6 :  56.8621063232422, 61.1525115966797, 67.6796340942383, 67.8479766845703, 75.6724319458008, 119.761451721191"

# Model Specifications

## Model 1
TODO: 
* One model with only the explanatory variables of key interest (possibly transformed, as determined by your EDA), and no other covariates.

$log(crmrte) = \beta_0 + \beta_1 \cdot log(density) + \beta_2 \cdot (taxpc) + u$

```{r}
m1 <- lm(log(crmrte) ~ log(density) + taxpc, data=data)
```

TODO: 
For your first model, a detailed assessment of the 6 CLM assumptions. For additional models, you should check all assumptions, but only highlight major differences from your first model in your report.

### CLM 1 - A linear model

The model is specified such that the dependent variable is a linear function of the explanatory variables. 
Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 2 - Random Sampling

TODO

### CLM 3 - Multicollinearity

```{r}
# show correlation
data$log.density <- log(data$density)
cor(data.matrix(subset(data, select=c("density", "log.density", "taxpc"))))
# show vif
vif(m1)
```

The two explanatory variables (log.density and taxpc) are not perfectly correlated and the VIFs are low (less than 4), so there is no perfect multicollinearity of the independent variables. 

Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 4 - Zero-Conditional Mean

```{r}
plot(m1, which=c(1, 5))
```

```{r}
# exogenous check
cov(log(data$density), m1$residuals)
cov(data$taxpc, m1$residuals)
```

The covariances of the two independent variables with the residuals are very close to zero indicating they are likely exogenous. 

There is a data point with a large Cook's distance. 

Is the assumption valid?

*Response*:

### CLM 5 - Homoscedasticity

```{r}
# spreadLevelPlot(m1)
plot(m1, which=3)
```

```{r}
bptest(m1)
```
not significant p-value for Breusch-Pagan test -> assumption of homoscedasticity met

```{r}
ncvTest(m1)
```
significant p-value for Score Test -> assumption of homoscedasticity met

Is the assumption valid? *Yes*

*Response*:

### CLM 6 - Normality of residuals

```{r}
# normality of standard residuals
standard <- rstandard(m1)
hist(standard, main="Histogram of standard resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(standard)), col="red", lwd=2, add=TRUE)
```

```{r}
# normality of studentized residuals
student <- rstudent(m1)
hist(student, main="Histogram of studentized resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(student)), col="red", lwd=2, add=TRUE)
```

```{r}
# QQ plot standard residuals
qqPlot(standard, distribution="norm", pch=20, main="QQ-Plot standard residuals")
qqline(standard, col="red", lwd=2)
```

```{r}
# QQ plot studentized residuals
qqPlot(student, distribution="norm", pch=20, main="QQ-Plot studentized residuals")
qqline(student, col="red", lwd=2)
```


The histograms in particular appear to be fairly normally distributed, while the Q-Q plot doesn’t deviate significantly from normality either, so overall the residuals appear to be fairly normal. Notably the log transformation of views has helped to produce normal
errors.

Is the assumption valid? *Yes*

Response: *No response required.*

TODO:
* One model that includes key explanatory variables and only covariates that you believe increase the accuracy of your results without introducing bias (for example, you should not include outcome variables that will absorb some of the causal effect you are interested in). This model should strike a balance between accuracy and parsimony and reflect your best understanding of the determinants of crime.

```{r}
m2 <- lm(log(crmrte) ~ log(density) + taxpc + urban, data=data)
```

### CLM 1 - A linear model

The model is specified such that the dependent variable is a linear function of the explanatory variables. 
Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 2 - Random Sampling

TODO

### CLM 3 - Multicollinearity

```{r}
# show correlation
data$log.density <- log(data$density)
cor(data.matrix(subset(data, select=c("density", "log.density", "taxpc", "urban"))))
# show vif
vif(m2)
```

The two explanatory variables (log.density and taxpc) are not perfectly correlated and the VIFs are low (less than 4), so there is no perfect multicollinearity of the independent variables. 

Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 4 - Zero-Conditional Mean

```{r}
plot(m2, which=c(1, 5))
```

```{r}
# exogenous check
cov(log(data$density), m2$residuals)
cov(data$taxpc, m2$residuals)

# TODO: figure out what to do with covariate
cov(data$urban, m2$residuals)
```

The covariances of the two independent variables with the residuals are very close to zero indicating they are likely exogenous. 

There is a data point with a large Cook's distance. 

Is the assumption valid?

*Response*:

### CLM 5 - Homoscedasticity

```{r}
# spreadLevelPlot(m1)
plot(m2, which=3)
```

```{r}
bptest(m2)
```
not significant p-value for Breusch-Pagan test -> assumption of homoscedasticity met

```{r}
ncvTest(m2)
```
significant p-value for Score Test -> assumption of homoscedasticity met

Is the assumption valid? *Yes*

*Response*:

### CLM 6 - Normality of residuals

```{r}
# normality of standard residuals
standard <- rstandard(m2)
hist(standard, main="Histogram of standard resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(standard)), col="red", lwd=2, add=TRUE)
```

```{r}
# normality of studentized residuals
student <- rstudent(m2)
hist(student, main="Histogram of studentized resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(student)), col="red", lwd=2, add=TRUE)
```

```{r}
# QQ plot standard residuals
qqPlot(standard, distribution="norm", pch=20, main="QQ-Plot standard residuals")
qqline(standard, col="red", lwd=2)
```

```{r}
# QQ plot studentized residuals
qqPlot(student, distribution="norm", pch=20, main="QQ-Plot studentized residuals")
qqline(student, col="red", lwd=2)
```

The histograms in particular appear to be fairly normally distributed, while the Q-Q plot doesn’t deviate significantly from normality either, so overall the residuals appear to be fairly normal. Notably the log transformation of views has helped to produce normal
errors.

Is the assumption valid? *Yes*

Response: *No response required.*


TODO: 
* One model that includes the previous covariates, and most, if not all, other covariates. A key purpose of this model is to demonstrate the robustness of your results to model specification.

```{r}
m3 <- lm(log(crmrte) ~ log(density) + taxpc + urban + central + west, data=data)
```

### CLM 1 - A linear model

The model is specified such that the dependent variable is a linear function of the explanatory variables. 
Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 2 - Random Sampling

TODO

### CLM 3 - Multicollinearity

```{r}
# show correlation
data$log.density <- log(data$density)
cor(data.matrix(subset(data, select=c("density", "log.density", "taxpc", "urban"))))
# show vif
vif(m2)
```

The two explanatory variables (log.density and taxpc) are not perfectly correlated and the VIFs are low (less than 4), so there is no perfect multicollinearity of the independent variables. 

Is the assumption valid? *Yes*

*Response*: No response required.

### CLM 4 - Zero-Conditional Mean

```{r}
plot(m2, which=c(1, 5))
```

```{r}
# exogenous check
cov(log(data$density), m2$residuals)
cov(data$taxpc, m2$residuals)

# TODO: figure out what to do with covariate
cov(data$urban, m2$residuals)
```

The covariances of the two independent variables with the residuals are very close to zero indicating they are likely exogenous. 

There is a data point with a large Cook's distance. 

Is the assumption valid?

*Response*:

### CLM 5 - Homoscedasticity

```{r}
# spreadLevelPlot(m1)
plot(m2, which=3)
```

```{r}
bptest(m2)
```
not significant p-value for Breusch-Pagan test -> assumption of homoscedasticity met

```{r}
ncvTest(m2)
```
significant p-value for Score Test -> assumption of homoscedasticity met

Is the assumption valid? *Yes*

*Response*:

### CLM 6 - Normality of residuals

```{r}
# normality of standard residuals
standard <- rstandard(m2)
hist(standard, main="Histogram of standard resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(standard)), col="red", lwd=2, add=TRUE)
```

```{r}
# normality of studentized residuals
student <- rstudent(m2)
hist(student, main="Histogram of studentized resiudals", freq=FALSE)
curve(dnorm(x, mean=0, sd=sd(student)), col="red", lwd=2, add=TRUE)
```

```{r}
# QQ plot standard residuals
qqPlot(standard, distribution="norm", pch=20, main="QQ-Plot standard residuals")
qqline(standard, col="red", lwd=2)
```

```{r}
# QQ plot studentized residuals
qqPlot(student, distribution="norm", pch=20, main="QQ-Plot studentized residuals")
qqline(student, col="red", lwd=2)
```

The histograms in particular appear to be fairly normally distributed, while the Q-Q plot doesn’t deviate significantly from normality either, so overall the residuals appear to be fairly normal. Notably the log transformation of views has helped to produce normal
errors.

Is the assumption valid? *Yes*

Response: *No response required.*

# Model Results

TODO: A well-formatted regression table summarizing your model results. Make sure that standard errors presented in this table are valid. Also, be sure to comment on both statistical and practical significance.

```{r}
se.m1 <- coef(summary(m1))[, "Std. Error"]
se.m2 <- coef(summary(m2))[, "Std. Error"]
se.m3 <- coef(summary(m3))[, "Std. Error"]
stargazer(m1, m2, m3, se=list(se.m1, se.m2, se.m3), omit.stat=c("f", "ser"), 
          star.cutoffs=c(0.05, 0.01, 0.001),
          type="latex", title="Linear Models Predicting Crime Rates")
```

# Causality

TODO: A detailed discussion of causality. In particular, include a discussion of what variables are not included in your analysis and the likely direction of omitted variable bias. Highlight any coefficients you find that appear to have the wrong sign from a causal perspective, and explain why this is the case.

# Conclusion

TODO: A brief conclusion with a few high-level takeaways.
